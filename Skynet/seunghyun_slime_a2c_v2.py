# -*- coding: utf-8 -*-
"""seunghyun_slime_a2c_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GqHNuIzVzYOR1Bi1Ayg7BNwUwl_eJRHe
"""

import gym
import slimevolleygym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
from gym.wrappers.monitoring.video_recorder import VideoRecorder
from collections import namedtuple

# Hyperparameters
gamma = 0.99
SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])
lr = 5e-4
env = gym.make("SlimeVolley-v0")

class Policy(nn.Module):

  def __init__(self, input_size, output_size):
    super(Policy, self).__init__()
    self.affine1 = nn.Linear(input_size, 128)
    self.action_head = nn.Linear(128, output_size)
    self.value_head = nn.Linear(128, 1)
    self.eps = np.finfo(np.float32).eps.item()
    self.saved_actions = []
    self.rewards = []

  def select_action(self, state):
    state = torch.from_numpy(state).float()
    probs, state_value = self(state)

    m = Categorical(probs)

    action = m.sample()

    self.saved_actions.append(SavedAction(m.log_prob(action), state_value))

    return action.item()

  def forward(self, x):

    x = F.relu(self.affine1(x))
    action_prob = F.softmax(self.action_head(x), dim=-1)
    state_values = self.value_head(x)

    return action_prob, state_values

model = Policy(12, 6)
optimizer = optim.Adam(model.parameters(), lr=lr)
eps = np.finfo(np.float32).eps.item()


def finish_episode():

  R = 0
  saved_actions = model.saved_actions
  policy_losses = []
  value_losses = []
  returns = []

  for r in model.rewards[::-1]:
    R = r + gamma * R
    returns.insert(0, R)

  returns = torch.tensor(returns)
  returns = (returns - returns.mean()) / (returns.std() + eps)


  for (log_prob, value), R in zip(saved_actions, returns):
    advantage = R - value.item()
    policy_losses.append(-log_prob * advantage)
    value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))

  optimizer.zero_grad()
  loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()

  loss.backward()
  optimizer.step()

  del model.rewards[:]
  del model.saved_actions[:]

def main():

  action_table = [[0, 0, 0], # NOOP
                  [1, 0, 0], # LEFT (forward)
                  [1, 0, 1], # UPLEFT (forward jump)
                  [0, 0, 1], # UP (jump)
                  [0, 1, 1], # UPRIGHT (backward jump)
                  [0, 1, 0]] # RIGHT (backward)
  for episode in range(15000):

    state = env.reset()
    if episode % 500 == 0:
      video  = VideoRecorder(env, path="videos/video_{}.mp4".format(episode), enabled=True)


    while True:
      if episode % 500 == 0:
        env.render()
        video.capture_frame()
      action = select_action(model,state)

      next_state, reward, done, _ = env.step(action_table[action])
      model.rewards.append(reward)
      state = next_state
      if done:
        if episode % 500 == 0:
          video.close()
        break
    finish_episode()
  torch.save(model.state_dict(), 'volley_actor_v3.pth')
if __name__ == "__main__":
  main()